{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qZSYa4WHJuq"
   },
   "source": [
    "# Skip-gram (Word2Vec) Preprocessing for Cancer Text Classification\n",
    "\n",
    "This notebook trains a **Word2Vec Skip-gram** model on the cancer research text corpus and prepares\n",
    "embedding matrices and tokenized sequences ready for downstream classification models (GRU, LSTM, RNN).\n",
    "\n",
    "## Why Skip-gram for This Dataset?\n",
    "\n",
    "Skip-gram is particularly well-suited for biomedical text classification for several reasons:\n",
    "\n",
    "1. **Rare word performance**: Skip-gram learns better representations for infrequent words compared to CBOW.\n",
    "   Biomedical text contains many specialized, low-frequency terms (gene names, drug names, anatomical terms)\n",
    "   that are critical for distinguishing cancer types. Skip-gram treats each context-target pair independently,\n",
    "   giving rare words more gradient updates relative to their frequency.\n",
    "\n",
    "2. **Semantic relationships in medical vocabulary**: Skip-gram captures fine-grained semantic similarities.\n",
    "   For example, it can learn that \"thyroidectomy\" and \"lobectomy\" are related surgical procedures,\n",
    "   or that \"BRAF\" and \"RAS\" are both oncogenes, even when they appear in different contexts.\n",
    "\n",
    "3. **Small-to-medium corpus size**: With ~7,500 documents, our corpus is relatively small by NLP standards.\n",
    "   Skip-gram is known to outperform CBOW on smaller datasets because it creates more training examples\n",
    "   per word occurrence (one for each context word in the window).\n",
    "\n",
    "4. **Domain-specific embeddings**: Pre-trained general embeddings (GloVe, FastText) may not capture\n",
    "   biomedical semantics well. Training Skip-gram directly on our corpus produces embeddings tailored\n",
    "   to the cancer classification domain.\n",
    "\n",
    "### Output\n",
    "All preprocessed artifacts are saved to `../preprocesseddata/` for reuse across model notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3ji-Z1aHJus",
    "outputId": "b1da4c29-d47d-4d1c-a779-327e4db45da1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.0)\n",
      "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print('All imports successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad184522",
    "outputId": "d75a1f38-3b3d-4a08-84b8-8e681cbf06b7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Duplicates found: 0\n",
      "After cleanup: 7570 samples\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f'Duplicates found: {duplicates}')\n",
    "df = df.drop_duplicates()\n",
    "print(f'After cleanup: {len(df)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "448322e1",
    "outputId": "0d197ca5-3d75-4129-d955-1e17e4a2f4d8"
   },
   "outputs": [],
   "source": "file_path = '../alldata_1_for_kaggle.csv'\ndf = pd.read_csv(file_path, encoding='latin1')\ndf.columns = ['id', 'cancer_type', 'text'] + list(df.columns[3:])\n\nprint(f'Dataset shape: {df.shape}')\nprint(f'Columns: {df.columns.tolist()}')\nprint(f'\\nClass distribution:')\nprint(df['cancer_type'].value_counts())\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpFf5pNsHJut"
   },
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJ_kgOBbHJuv"
   },
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "We apply the same preprocessing pipeline used in the GRU classification notebook to ensure consistency\n",
    "across all team members' model inputs:\n",
    "- Lowercasing\n",
    "- URL and email removal\n",
    "- Special character removal (keeping only alphabetic characters)\n",
    "- Stopword removal\n",
    "- Lemmatization\n",
    "- Filtering tokens shorter than 3 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dVaS6uflHJuv",
    "outputId": "f16cf86a-b336-4079-f583-d8e626953869"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample:\n",
      "Original: Thyroid surgery in  children in a single institution from Osama Ibrahim Almosallama Ali Aseerib Ahme ...\n",
      "Cleaned : thyroid surgery child single institution osama ibrahim almosallama ali aseerib ahmed alhumaida ali a ...\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and tokenize text for embedding training.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print('Sample:')\n",
    "print('Original:', df['text'].iloc[0][:100], '...')\n",
    "print('Cleaned :', preprocess_text(df['text'].iloc[0])[:100], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l00LQF-9HJuw",
    "outputId": "1c065964-5580-493e-ae0f-5856b45de052"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preprocessing texts...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 7570/7570 [02:57<00:00, 42.77it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final samples: 7570\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "print('Preprocessing texts...')\n",
    "df['cleaned_text'] = df['text'].progress_apply(preprocess_text)\n",
    "df = df[df['cleaned_text'].str.strip() != '']\n",
    "print(f'Final samples: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPRlZtSyHJuw"
   },
   "source": [
    "## 3. Label Encoding & Train/Val/Test Split\n",
    "\n",
    "Using the same split ratios and random state as the GRU notebook for fair comparison:\n",
    "- 70% train, 15% validation, 15% test\n",
    "- Stratified split to preserve class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80Ag6qCXHJuw",
    "outputId": "d6c82368-df0d-4714-9867-be10b9252275"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label mapping:\n",
      "  Colon_Cancer -> 0\n",
      "  Lung_Cancer -> 1\n",
      "  Thyroid_Cancer -> 2\n",
      "\n",
      "Train: 5301, Val: 1133, Test: 1136\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['cancer_type'])\n",
    "\n",
    "print('Label mapping:')\n",
    "for i, name in enumerate(label_encoder.classes_):\n",
    "    print(f'  {name} -> {i}')\n",
    "\n",
    "X = df['cleaned_text'].values\n",
    "y = df['label'].values\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f'\\nTrain: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtLtdHJPHJux"
   },
   "source": [
    "## 4. Train Word2Vec Skip-gram Model\n",
    "\n",
    "### Skip-gram Architecture\n",
    "\n",
    "Skip-gram predicts surrounding context words given a center word. For each word in the corpus,\n",
    "it creates training pairs (center_word, context_word) within a sliding window.\n",
    "\n",
    "**Key hyperparameters chosen for this dataset:**\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|-----------|\n",
    "| `vector_size=100` | Matches GRU embedding dimension for direct comparison |\n",
    "| `window=5` | Captures local context typical of scientific writing |\n",
    "| `min_count=2` | Keeps rare but meaningful biomedical terms (appearing at least twice) |\n",
    "| `sg=1` | Selects Skip-gram (vs CBOW which uses sg=0) |\n",
    "| `negative=5` | Negative sampling for efficient training |\n",
    "| `epochs=20` | Sufficient convergence for our corpus size |\n",
    "| `workers=4` | Parallel training threads |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NaVq1bO8HJux",
    "outputId": "467431c6-f6e0-4c45-b787-723058050ba3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Word2Vec Skip-gram on 5301 documents...\n",
      "Total tokens in training set: 11,451,874\n",
      "\n",
      "Skip-gram model trained!\n",
      "Vocabulary size (Word2Vec): 162502\n",
      "Embedding dimension: 100\n"
     ]
    }
   ],
   "source": [
    "# Tokenize training texts into lists of words for Word2Vec\n",
    "train_tokenized = [text.split() for text in X_train]\n",
    "\n",
    "print(f'Training Word2Vec Skip-gram on {len(train_tokenized)} documents...')\n",
    "print(f'Total tokens in training set: {sum(len(doc) for doc in train_tokenized):,}')\n",
    "\n",
    "skipgram_model = Word2Vec(\n",
    "    sentences=train_tokenized,\n",
    "    vector_size=100,    # embedding dimension\n",
    "    window=5,           # context window size\n",
    "    min_count=5,        # minimum word frequency (changed from 2 to 5)\n",
    "    sg=1,               # 1 = Skip-gram, 0 = CBOW\n",
    "    negative=5,         # negative sampling\n",
    "    epochs=10,          # training iterations (changed from 20 to 10)\n",
    "    workers=4,          # parallel threads\n",
    "    seed=42             # reproducibility\n",
    ")\n",
    "\n",
    "print(f'\\nSkip-gram model trained!')\n",
    "print(f'Vocabulary size (Word2Vec): {len(skipgram_model.wv)}')\n",
    "print(f'Embedding dimension: {skipgram_model.wv.vector_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "T23i1FYdHJux",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e32f2f04-ec33-4c1b-f029-fde2cb2b1d0a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Most similar words learned by Skip-gram:\n",
      "============================================================\n",
      "  cancer       -> breast (0.780), prostate (0.683), lung (0.639), nonsmall (0.634), colorectal (0.631)\n",
      "  tumor        -> tumour (0.647), microenvironment (0.637), teaming (0.606), malignant (0.600), solid (0.598)\n",
      "  thyroid      -> papillary (0.745), carcinomapapillary (0.715), carcinomaneeds (0.708), associationguidelines (0.702), dierentiatedthyroid (0.701)\n",
      "  colon        -> secondtumor (0.660), andrectum (0.656), betteroutcomes (0.656), rectum (0.654), modelsbreast (0.651)\n",
      "  lung         -> nonsmall (0.759), nonsmallcell (0.679), nsclc (0.641), cancer (0.639), thoraconcol (0.637)\n",
      "  patient      -> patientswith (0.735), nftdtpi (0.674), npoor (0.667), inmedocc (0.645), composingof (0.644)\n",
      "  treatment    -> therapy (0.684), chemotherapy (0.610), treated (0.592), effective (0.592), modality (0.571)\n",
      "  cell         -> proliferation (0.692), migration (0.668), migation (0.663), apoptosis (0.663), bothpdac (0.660)\n"
     ]
    }
   ],
   "source": [
    "# Explore learned embeddings - check if Skip-gram captured meaningful medical relationships\n",
    "test_words = ['cancer', 'tumor', 'thyroid', 'colon', 'lung', 'patient', 'treatment', 'cell']\n",
    "\n",
    "print('Most similar words learned by Skip-gram:')\n",
    "print('=' * 60)\n",
    "for word in test_words:\n",
    "    if word in skipgram_model.wv:\n",
    "        similar = skipgram_model.wv.most_similar(word, topn=5)\n",
    "        similar_str = ', '.join([f'{w} ({s:.3f})' for w, s in similar])\n",
    "        print(f'  {word:12s} -> {similar_str}')\n",
    "    else:\n",
    "        print(f'  {word:12s} -> [not in vocabulary]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70vANqg2HJux"
   },
   "source": [
    "## 5. Build Vocabulary & Embedding Matrix\n",
    "\n",
    "We construct a vocabulary of the top 5,000 most frequent words (matching the GRU notebook's `MAX_WORDS`)\n",
    "and build a pre-trained embedding matrix from the Skip-gram vectors. Words not in the Skip-gram\n",
    "vocabulary are initialized to zero (they will be mapped to the `<UNK>` token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "isOLzJhXHJuy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "84ffff07-5159-4e0f-98b1-c177677bdfa8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 5000\n",
      "Max sequence length: 300\n",
      "Embedding dimension: 100\n"
     ]
    }
   ],
   "source": [
    "MAX_WORDS = 5000\n",
    "MAX_LEN = 300\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Build vocabulary from training data (same as GRU notebook)\n",
    "word_counts = Counter()\n",
    "for text in X_train:\n",
    "    word_counts.update(text.split())\n",
    "\n",
    "vocab = ['<PAD>', '<UNK>'] + [w for w, c in word_counts.most_common(MAX_WORDS - 2)]\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "print(f'Max sequence length: {MAX_LEN}')\n",
    "print(f'Embedding dimension: {EMBEDDING_DIM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "CTudYzfpHJuy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6c0b0879-c6c6-45f4-e3f3-3a890ed7e487"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Embedding matrix shape: (5000, 100)\n",
      "Words with Skip-gram vectors: 4998/5000 (100.0%)\n",
      "Words without vectors (will use zeros): 2\n"
     ]
    }
   ],
   "source": [
    "# Build pre-trained embedding matrix from Skip-gram vectors\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "found_count = 0\n",
    "missing_count = 0\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    if word in skipgram_model.wv:\n",
    "        embedding_matrix[idx] = skipgram_model.wv[word]\n",
    "        found_count += 1\n",
    "    else:\n",
    "        missing_count += 1\n",
    "\n",
    "coverage = found_count / vocab_size * 100\n",
    "print(f'Embedding matrix shape: {embedding_matrix.shape}')\n",
    "print(f'Words with Skip-gram vectors: {found_count}/{vocab_size} ({coverage:.1f}%)')\n",
    "print(f'Words without vectors (will use zeros): {missing_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zvs6X2SZHJuy"
   },
   "source": [
    "## 6. Convert Texts to Sequences\n",
    "\n",
    "Each document is converted to a fixed-length integer sequence:\n",
    "- Words are mapped to their vocabulary index\n",
    "- Unknown words map to index 1 (`<UNK>`)\n",
    "- Sequences are truncated or zero-padded to `MAX_LEN=300`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "soEM37OhHJuy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5ea66c3a-4945-4ddc-bad1-81f3dad05b46"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequence shapes:\n",
      "  Train: (5301, 300)\n",
      "  Val:   (1133, 300)\n",
      "  Test:  (1136, 300)\n"
     ]
    }
   ],
   "source": [
    "def text_to_sequence(text, word2idx, max_len):\n",
    "    \"\"\"Convert text to padded integer sequence.\"\"\"\n",
    "    tokens = text.split()[:max_len]\n",
    "    seq = [word2idx.get(w, 1) for w in tokens]  # 1 = <UNK>\n",
    "    seq = seq + [0] * (max_len - len(seq))       # 0 = <PAD>\n",
    "    return seq\n",
    "\n",
    "X_train_seq = np.array([text_to_sequence(t, word2idx, MAX_LEN) for t in X_train])\n",
    "X_val_seq = np.array([text_to_sequence(t, word2idx, MAX_LEN) for t in X_val])\n",
    "X_test_seq = np.array([text_to_sequence(t, word2idx, MAX_LEN) for t in X_test])\n",
    "\n",
    "print(f'Sequence shapes:')\n",
    "print(f'  Train: {X_train_seq.shape}')\n",
    "print(f'  Val:   {X_val_seq.shape}')\n",
    "print(f'  Test:  {X_test_seq.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc_6CNgQHJuy"
   },
   "source": [
    "## 7. Save All Preprocessed Data\n",
    "\n",
    "Saving the following artifacts to `../preprocesseddata/`:\n",
    "\n",
    "| File | Contents |\n",
    "|------|----------|\n",
    "| `skipgram_embedding_matrix.npy` | Pre-trained embedding matrix (5000 x 100) |\n",
    "| `skipgram_sequences.npz` | Train/val/test integer sequences |\n",
    "| `skipgram_labels.npz` | Train/val/test labels |\n",
    "| `skipgram_vocab.pkl` | Vocabulary mappings (word2idx, idx2word) |\n",
    "| `skipgram_model.model` | Trained Word2Vec model (for further analysis) |\n",
    "| `skipgram_config.pkl` | Hyperparameters and metadata |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "wrM8WQxOHJuy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b73d23ea-c1d4-4452-c200-907c4274be2e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved: skipgram_embedding_matrix.npy  (5000, 100)\n",
      "Saved: skipgram_sequences.npz\n",
      "Saved: skipgram_labels.npz\n",
      "Saved: skipgram_vocab.pkl\n",
      "Saved: skipgram_word2vec.model\n",
      "Saved: skipgram_config.pkl\n",
      "\n",
      "All files saved to ../preprocesseddata/\n"
     ]
    }
   ],
   "source": [
    "output_dir = '../preprocesseddata'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Save embedding matrix\n",
    "np.save(os.path.join(output_dir, 'skipgram_embedding_matrix.npy'), embedding_matrix)\n",
    "print(f'Saved: skipgram_embedding_matrix.npy  {embedding_matrix.shape}')\n",
    "\n",
    "# 2. Save sequences\n",
    "np.savez(os.path.join(output_dir, 'skipgram_sequences.npz'),\n",
    "         X_train=X_train_seq, X_val=X_val_seq, X_test=X_test_seq)\n",
    "print(f'Saved: skipgram_sequences.npz')\n",
    "\n",
    "# 3. Save labels\n",
    "np.savez(os.path.join(output_dir, 'skipgram_labels.npz'),\n",
    "         y_train=y_train, y_val=y_val, y_test=y_test)\n",
    "print(f'Saved: skipgram_labels.npz')\n",
    "\n",
    "# 4. Save vocabulary\n",
    "with open(os.path.join(output_dir, 'skipgram_vocab.pkl'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'word2idx': word2idx,\n",
    "        'idx2word': idx2word,\n",
    "        'vocab': vocab,\n",
    "        'label_encoder': label_encoder\n",
    "    }, f)\n",
    "print(f'Saved: skipgram_vocab.pkl')\n",
    "\n",
    "# 5. Save trained Word2Vec model\n",
    "skipgram_model.save(os.path.join(output_dir, 'skipgram_word2vec.model'))\n",
    "print(f'Saved: skipgram_word2vec.model')\n",
    "\n",
    "# 6. Save config/metadata\n",
    "config = {\n",
    "    'MAX_WORDS': MAX_WORDS,\n",
    "    'MAX_LEN': MAX_LEN,\n",
    "    'EMBEDDING_DIM': EMBEDDING_DIM,\n",
    "    'vocab_size': vocab_size,\n",
    "    'num_classes': len(label_encoder.classes_),\n",
    "    'class_names': list(label_encoder.classes_),\n",
    "    'train_size': len(X_train),\n",
    "    'val_size': len(X_val),\n",
    "    'test_size': len(X_test),\n",
    "    'skipgram_params': {\n",
    "        'vector_size': 100,\n",
    "        'window': 5,\n",
    "        'min_count': 2,\n",
    "        'sg': 1,\n",
    "        'negative': 5,\n",
    "        'epochs': 20\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(output_dir, 'skipgram_config.pkl'), 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "print(f'Saved: skipgram_config.pkl')\n",
    "\n",
    "print(f'\\nAll files saved to {output_dir}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JPUuj4VHJuy"
   },
   "source": [
    "## 8. Verification - Load and Check Saved Data\n",
    "\n",
    "Quick sanity check to confirm everything was saved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "eG0iXR7THJuy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bf495a38-cdb1-4215-a009-00fca1130695"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Verifying saved data...\n",
      "==================================================\n",
      "Embedding matrix: (5000, 100)\n",
      "Train sequences: (5301, 300)\n",
      "Val sequences:   (1133, 300)\n",
      "Test sequences:  (1136, 300)\n",
      "Train labels: (5301,)\n",
      "Val labels:   (1133,)\n",
      "Test labels:  (1136,)\n",
      "Vocab size: 5000\n",
      "Classes: ['Colon_Cancer', 'Lung_Cancer', 'Thyroid_Cancer']\n",
      "\n",
      "Config: {'MAX_WORDS': 5000, 'MAX_LEN': 300, 'EMBEDDING_DIM': 100, 'vocab_size': 5000, 'num_classes': 3, 'class_names': ['Colon_Cancer', 'Lung_Cancer', 'Thyroid_Cancer'], 'train_size': 5301, 'val_size': 1133, 'test_size': 1136, 'skipgram_params': {'vector_size': 100, 'window': 5, 'min_count': 2, 'sg': 1, 'negative': 5, 'epochs': 20}}\n",
      "\n",
      "All verifications passed!\n"
     ]
    }
   ],
   "source": [
    "# Verify saved files\n",
    "print('Verifying saved data...')\n",
    "print('=' * 50)\n",
    "\n",
    "# Load and check embedding matrix\n",
    "emb = np.load(os.path.join(output_dir, 'skipgram_embedding_matrix.npy'))\n",
    "print(f'Embedding matrix: {emb.shape}')\n",
    "\n",
    "# Load and check sequences\n",
    "seqs = np.load(os.path.join(output_dir, 'skipgram_sequences.npz'))\n",
    "print(f'Train sequences: {seqs[\"X_train\"].shape}')\n",
    "print(f'Val sequences:   {seqs[\"X_val\"].shape}')\n",
    "print(f'Test sequences:  {seqs[\"X_test\"].shape}')\n",
    "\n",
    "# Load and check labels\n",
    "labels = np.load(os.path.join(output_dir, 'skipgram_labels.npz'))\n",
    "print(f'Train labels: {labels[\"y_train\"].shape}')\n",
    "print(f'Val labels:   {labels[\"y_val\"].shape}')\n",
    "print(f'Test labels:  {labels[\"y_test\"].shape}')\n",
    "\n",
    "# Load and check vocab\n",
    "with open(os.path.join(output_dir, 'skipgram_vocab.pkl'), 'rb') as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "print(f'Vocab size: {len(vocab_data[\"word2idx\"])}')\n",
    "print(f'Classes: {list(vocab_data[\"label_encoder\"].classes_)}')\n",
    "\n",
    "# Load config\n",
    "with open(os.path.join(output_dir, 'skipgram_config.pkl'), 'rb') as f:\n",
    "    cfg = pickle.load(f)\n",
    "print(f'\\nConfig: {cfg}')\n",
    "\n",
    "print('\\nAll verifications passed!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "history_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}